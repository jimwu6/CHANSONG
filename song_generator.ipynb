{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "song_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimwu6/rnn-genius/blob/master/song_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdKHp9lZdoro"
      },
      "source": [
        "# RNN Genius\r\n",
        "\r\n",
        "\r\n",
        "Resources:\r\n",
        "* [Lab Notebook Doc](https://docs.google.com/document/d/1yXhX0KPZ_Z4c9jY0r8-wTuBfnI-qHExfJMpQCCfy6zs/edit#)\r\n",
        "* [Original inspiration](http://karpathy.github.io/2015/05/21/rnn-effectiveness/?fbclid=IwAR2jDIjSieoc9ZtG_7FLN03Q3LZcUUtkw7V_4mnW0pNrelXoi6PAQsO2ffQ)\r\n",
        "* [Keras for RNNs/LSTMs](https://www.tensorflow.org/guide/keras/rnn)\r\n",
        "* [Text Classification](https://www.tensorflow.org/tutorials/text/text_classification_rnn)\r\n",
        "* [Text Generation](https://www.tensorflow.org/tutorials/text/text_generation) - Code was extrapolated from here.\r\n",
        "\r\n",
        "### Future Plans:\r\n",
        "* Move webapp to cloud\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlYU11nkjQ4o"
      },
      "source": [
        "## Setup\r\n",
        "\r\n",
        "We suggest enabling the GPU or TPU if you would like to train models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNqxs7WPk2iN"
      },
      "source": [
        "### Import the libraries\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9rER-GUZMLG"
      },
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "import time\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.layers.experimental import preprocessing\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXHFikn3wHa-"
      },
      "source": [
        "### Define constants\r\n",
        "Some Resources:\r\n",
        "* [Embedding](https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMOT4goPwKL_"
      },
      "source": [
        "# Batches\r\n",
        "SEQ_LENGTH = 500\r\n",
        "BATCH_SIZE = 64\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "# Model \r\n",
        "EMBEDDING_DIM = 256\r\n",
        "RNN_UNITS = 768\r\n",
        "\r\n",
        "# Train\r\n",
        "EPOCHS = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grdA9rWpeOfX"
      },
      "source": [
        "### Get dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HJ_gAmdg1z5"
      },
      "source": [
        "# get file path for input\r\n",
        "# path_to_file = keras.utils.get_file('lyrics.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\r\n",
        "path_to_file = \"/content/DS_2.txt\"\r\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJfky-Dclr2F"
      },
      "source": [
        "## Text Processing\r\n",
        "\r\n",
        "To encode the words, we will use the sorted list of characters that exist in the text.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOpq6l0Xo9K7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48299bd-1762-48b3-9011-ef64db655559"
      },
      "source": [
        "# CHECK TEXT\r\n",
        "print('Length of text: {} characters'.format(len(text)))\r\n",
        "#print(text[:1000])\r\n",
        "\r\n",
        "# CHECK UNIQUE CHARS\r\n",
        "vocab = sorted(set(text))\r\n",
        "print('{} unique characters'.format(len(vocab)))\r\n",
        "print(vocab)\r\n",
        "\r\n",
        "VOCAB_SIZE = len(vocab)\r\n",
        "\r\n",
        "# CHAR ENCODING TO ID\r\n",
        "chars = tf.strings.unicode_split(text, input_encoding='UTF-8')\r\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\r\n",
        "ids = ids_from_chars(chars)\r\n",
        "\r\n",
        "# INVERSION TO CHAR\r\n",
        "chars_from_ids = preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\r\n",
        "\r\n",
        "def text_from_ids(ids):\r\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 20467159 characters\n",
            "84 unique characters\n",
            "['\\n', '\\r', ' ', '!', '\"', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPvDNN6vTvE1"
      },
      "source": [
        "### Training examples & targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQwxcW3CSWiG",
        "outputId": "4b23d782-6535-41d3-c06f-439b50a308c2"
      },
      "source": [
        "def split_input_target(sequence):\r\n",
        "    input_text = sequence[:-1]\r\n",
        "    target_text = sequence[1:]\r\n",
        "    return input_text, target_text\r\n",
        "\r\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(ids)\r\n",
        "\r\n",
        "sequences = ids_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)\r\n",
        "\r\n",
        "for input_example, target_example in dataset.take(1):\r\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\r\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'[Intro]\\r\\nHmm\\r\\n\\r\\n[Verse 1]\\r\\nYou might think I\\'m crazy\\r\\nThe way I\\'ve been cravin\\'\\r\\nIf I put it quite plainly\\r\\nJust gimme them babies\\r\\nSo what you doin\\' tonight?\\r\\nBetter say, \"Doin\\' you right\" (Yeah)\\r\\nWatchin\\' movies, but we ain\\'t seen a thing tonight (Yeah)\\r\\n\\r\\n[Pre-Chorus]\\r\\nI don\\'t wanna keep you up (You up)\\r\\nBut show me, can you keep it up? (It up)\\r\\n\\'Cause then I\\'ll have to keep you up\\r\\nShit, maybe I\\'ma keep you up, boy\\r\\nI\\'ve been drinkin\\' coffee (I\\'ve been drinkin\\' coffee; coffee)\\r\\nAnd I\\'ve been'\n",
            "Target: b'Intro]\\r\\nHmm\\r\\n\\r\\n[Verse 1]\\r\\nYou might think I\\'m crazy\\r\\nThe way I\\'ve been cravin\\'\\r\\nIf I put it quite plainly\\r\\nJust gimme them babies\\r\\nSo what you doin\\' tonight?\\r\\nBetter say, \"Doin\\' you right\" (Yeah)\\r\\nWatchin\\' movies, but we ain\\'t seen a thing tonight (Yeah)\\r\\n\\r\\n[Pre-Chorus]\\r\\nI don\\'t wanna keep you up (You up)\\r\\nBut show me, can you keep it up? (It up)\\r\\n\\'Cause then I\\'ll have to keep you up\\r\\nShit, maybe I\\'ma keep you up, boy\\r\\nI\\'ve been drinkin\\' coffee (I\\'ve been drinkin\\' coffee; coffee)\\r\\nAnd I\\'ve been '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRwlXP_AYBZo"
      },
      "source": [
        "### Batch dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--OBzcWHYKT0"
      },
      "source": [
        "dataset = (\r\n",
        "    dataset\r\n",
        "    .shuffle(BUFFER_SIZE)\r\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPH-8jLlY_og"
      },
      "source": [
        "## Building Model\r\n",
        "We used four different model archetictures (not including specific unit numbers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jmPRF8vZJyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df850b94-8d4b-48c8-f4cc-9f9a96928975"
      },
      "source": [
        "# Embedding -> GRU -> Dense\r\n",
        "\r\n",
        "class ModelOne(keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\r\n",
        "        super().__init__(self)\r\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units,\r\n",
        "                                    return_sequences=True, \r\n",
        "                                    return_state=True,\r\n",
        "                                    )\r\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\r\n",
        "        x = inputs\r\n",
        "        x = self.embedding(x, training=training)\r\n",
        "        if states is None:\r\n",
        "            states = self.gru.get_initial_state(x)\r\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\r\n",
        "        x = self.dense(x, training=training)\r\n",
        "\r\n",
        "        if return_state:\r\n",
        "            return x, states\r\n",
        "        else: \r\n",
        "            return x\r\n",
        "\r\n",
        "model = ModelOne(\r\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\r\n",
        "    embedding_dim=EMBEDDING_DIM,\r\n",
        "    rnn_units=RNN_UNITS)\r\n",
        "\r\n",
        "for input_batch, target_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 86) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bztr80s9wAcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287bcc4c-8eae-4253-cada-26b7052a54f1"
      },
      "source": [
        "# Embedding -> GRU -> GRU -> Dense\r\n",
        "\r\n",
        "class ModelTwo(keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\r\n",
        "        super().__init__(self)\r\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "        self.gru_one = tf.keras.layers.GRU(rnn_units,\r\n",
        "                                    return_sequences=True, \r\n",
        "                                    return_state=True)\r\n",
        "        self.gru_two = tf.keras.layers.GRU(rnn_units,\r\n",
        "                                    return_sequences=True, \r\n",
        "                                    return_state=True)\r\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\r\n",
        "        x = inputs\r\n",
        "        x = self.embedding(x, training=training)\r\n",
        "        if states is None:\r\n",
        "            states = [self.gru_one.get_initial_state(x), self.gru_two.get_initial_state(x)]\r\n",
        "        x, states[0] = self.gru_one(x, initial_state=states[0], training=training)\r\n",
        "        x, states[1] = self.gru_two(x, initial_state=states[1], training=training)\r\n",
        "        x = self.dense(x, training=training)\r\n",
        "\r\n",
        "        if return_state:\r\n",
        "            return x, states\r\n",
        "        else: \r\n",
        "            return x\r\n",
        "\r\n",
        "model = ModelTwo(\r\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\r\n",
        "    embedding_dim=EMBEDDING_DIM,\r\n",
        "    rnn_units=RNN_UNITS)\r\n",
        "\r\n",
        "for input_batch, target_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 300, 86) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VO8_buxwLtN"
      },
      "source": [
        "# Embedding -> LSTM -> Dense\r\n",
        "\r\n",
        "class ModelThree(keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\r\n",
        "        super().__init__(self)\r\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "        self.lstm = tf.keras.layers.LSTM(rnn_units,\r\n",
        "                                    return_sequences=True, \r\n",
        "                                    return_state=True\r\n",
        "                                    )\r\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\r\n",
        "        x = inputs\r\n",
        "        x = self.embedding(x, training=training)\r\n",
        "        # if states is None:\r\n",
        "        #     states = [tf.zeros([64,768]), tf.zeros([64, 768])]\r\n",
        "        x, h, c = self.lstm(x, initial_state=states, training=training)\r\n",
        "        states = [h,c]\r\n",
        "        x = self.dense(x, training=training)\r\n",
        "\r\n",
        "        if return_state:\r\n",
        "            return x, states\r\n",
        "        else: \r\n",
        "            return x\r\n",
        "\r\n",
        "model = ModelThree(\r\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\r\n",
        "    embedding_dim=EMBEDDING_DIM,\r\n",
        "    rnn_units=RNN_UNITS)\r\n",
        "\r\n",
        "for input_batch, target_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DprBaW161QPx",
        "outputId": "07652ce8-932d-45ce-b1b8-6773270fc3d1"
      },
      "source": [
        "# Embedding -> LSTM -> LSTM -> Dense\r\n",
        "\r\n",
        "class ModelFour(keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\r\n",
        "        super().__init__(self)\r\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "        self.lstm_one = tf.keras.layers.LSTM(rnn_units,\r\n",
        "                                    return_sequences=True, \r\n",
        "                                    return_state=True\r\n",
        "                                    )\r\n",
        "        self.lstm_two = tf.keras.layers.LSTM(rnn_units,\r\n",
        "                                    return_sequences=True, \r\n",
        "                                    return_state=True\r\n",
        "                                    )\r\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\r\n",
        "        x = inputs\r\n",
        "        x = self.embedding(x, training=training)\r\n",
        "        if states is None:\r\n",
        "            states = [None, None]\r\n",
        "        x, h, c = self.lstm_one(x, initial_state=states[0], training=training)\r\n",
        "        states[0] = [h,c]\r\n",
        "        x, h, c = self.lstm_two(x, initial_state=states[1], training=training)\r\n",
        "        states[1] = [h,c]\r\n",
        "        x = self.dense(x, training=training)\r\n",
        "\r\n",
        "        if return_state:\r\n",
        "            return x, states\r\n",
        "        else: \r\n",
        "            return x\r\n",
        "\r\n",
        "model = ModelFour(\r\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\r\n",
        "    embedding_dim=EMBEDDING_DIM,\r\n",
        "    rnn_units=RNN_UNITS)\r\n",
        "\r\n",
        "for input_batch, target_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 500, 86) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoFP64PTwim1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIFRg5VbP57"
      },
      "source": [
        "### Testing stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vn80T6PbRm1"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\r\n",
        "print(\"Input:\\n\", text_from_ids(input_batch[0]).numpy())\r\n",
        "print()\r\n",
        "print(\"Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Teovdrkcb4p"
      },
      "source": [
        "## Training Model\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i_WimtEdDc3"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "\r\n",
        "example_batch_loss = loss(target_batch, example_batch_predictions)\r\n",
        "mean_loss = example_batch_loss.numpy().mean()\r\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n",
        "print(\"Mean loss:        \", mean_loss)\r\n",
        "\r\n",
        "model.compile(optimizer='adam', loss=loss)\r\n",
        "\r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix, \r\n",
        "    save_weights_only=True, \r\n",
        "    save_freq= 5 * (len(sequences) // BATCH_SIZE),\r\n",
        "    verbose=1\r\n",
        ") # save_freq for how many checkpoints (every 5 epochs)\r\n",
        "\r\n",
        "# Train\r\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTIAqMUTd_0I"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-LxCSWCeB5D"
      },
      "source": [
        "class OneStep(keras.Model):\r\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.85):\r\n",
        "        super().__init__()\r\n",
        "        self.temperature = temperature\r\n",
        "        self.model = model\r\n",
        "        self.chars_from_ids = chars_from_ids\r\n",
        "        self.ids_from_chars = ids_from_chars\r\n",
        "\r\n",
        "        # Create mask to prevent \"\" and \"[UNK]\"\r\n",
        "        skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\r\n",
        "        sparse_mask = tf.SparseTensor(\r\n",
        "            values = [-float('inf')] * len(skip_ids),\r\n",
        "            indices = skip_ids,\r\n",
        "            dense_shape = [len(ids_from_chars.get_vocabulary())]\r\n",
        "        )\r\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def generate_one_step(self, inputs, states=None):\r\n",
        "        # Convert strings to tokens\r\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\r\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\r\n",
        "        print((self.ids_from_chars(input_chars)).shape)\r\n",
        "        # Run model\r\n",
        "        if (type(states) == type([])):\r\n",
        "          predicted_logits, states = self.model(inputs=input_ids, states=states[:], return_state=True)\r\n",
        "        else:\r\n",
        "          predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\r\n",
        "\r\n",
        "        # Only use the last prediction\r\n",
        "        predicted_logits = predicted_logits[:, -1, :]\r\n",
        "        predicted_logits = predicted_logits / self.temperature\r\n",
        "        # Apply prediction mask\r\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\r\n",
        "\r\n",
        "        # Generate IDs\r\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\r\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\r\n",
        "\r\n",
        "        # Convert to chars\r\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\r\n",
        "\r\n",
        "        # Return chars, model state\r\n",
        "        return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ33TWFjg8mP"
      },
      "source": [
        "### Testing train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IaVdIm6g-ck"
      },
      "source": [
        "# Set the temperature (usually it is in (0, 1])\r\n",
        "temperature = 0.85\r\n",
        "\r\n",
        "# Set the starting text\r\n",
        "next_char = tf.constant(['[Intro]'])\r\n",
        "\r\n",
        "# Set how many characters to generate\r\n",
        "length = 1000\r\n",
        "\r\n",
        "# Create the model to test\r\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=temperature)\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "states = None\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "# Generate lyrics\r\n",
        "for n in range(length):\r\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
        "    result.append(next_char)\r\n",
        "\r\n",
        "result = tf.strings.join(result)\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "# Print generated lyrics\r\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*72)\r\n",
        "print(f\"\\nRun time: {end - start}\")\r\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFk6F6YZhSKa"
      },
      "source": [
        "### Visualizers\r\n",
        "\r\n",
        "Only loss is being tracked, so we will plot it from the training history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prVhcl35hRi0"
      },
      "source": [
        "plt.plot(history.history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjrPv2TEcOlu"
      },
      "source": [
        "# Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTDYNpkOdYLB"
      },
      "source": [
        "## Loading weights\r\n",
        "If you have weights saved, you can load them in two different ways but it will only work if the weights are the same size as your current model vocabulary. Otherwise you can initialize a new model if you know the vocabulary size.\r\n",
        "\r\n",
        "Resources:\r\n",
        "* [Save and load models](https://www.tensorflow.org/tutorials/keras/save_and_load)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xStHoSlaYowM"
      },
      "source": [
        "# # LOAD PREVIOUS WEIGHTS IF YOU HAVE THEM\r\n",
        "checkpoint_dir = './' # change the directory as needed\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_30\") # only if you want to load specific weights\r\n",
        "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) \r\n",
        "model.load_weights(checkpoint_prefix)\r\n",
        "\r\n",
        "# LOAD FROM GCLOUD\r\n",
        "# from google.colab import auth\r\n",
        "# from tensorflow.python.lib.io import file_io\r\n",
        "\r\n",
        "# auth.authenticate_user()\r\n",
        "\r\n",
        "# model_file = file_io.FileIO('GCLOUD BUCKET DIRECTORY', mode='rb')\r\n",
        "\r\n",
        "# temp_model_location = './model21_checkpoints/ckpt_30.index'\r\n",
        "# temp_model_file = open(temp_model_location, 'wb')\r\n",
        "# temp_model_file.write(model_file.read())\r\n",
        "# temp_model_file.close()\r\n",
        "# model_file.close()\r\n",
        "\r\n",
        "# model_file = file_io.FileIO('gs://rnn-genius-models-bucket/model21/training_checkpoints/ckpt_30.data-00000-of-00001', mode='rb')\r\n",
        "\r\n",
        "# temp_model_location = './model21_checkpoints/ckpt_30.data-00000-of-00001'\r\n",
        "# temp_model_file = open(temp_model_location, 'wb')\r\n",
        "# temp_model_file.write(model_file.read())\r\n",
        "# temp_model_file.close()\r\n",
        "# model_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpsPrXwtk4kP"
      },
      "source": [
        "### Train More Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5izq3N7Ak7vg"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "model.compile(optimizer='adam', loss=loss)\r\n",
        "\r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix, \r\n",
        "    save_weights_only=True, \r\n",
        "    save_freq=5 * (len(sequences) // BATCH_SIZE),\r\n",
        "    verbose=1\r\n",
        ") # save_freq for how many checkpoints (every 5 epochs)\r\n",
        "\r\n",
        "EPOCHS = 30\r\n",
        "\r\n",
        "history2 = model.fit(dataset, epochs=EPOCHS, initial_epoch=EPOCHS, callbacks=[checkpoint_callback])\r\n",
        "plt.plot(history2.history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiV8d_2idV_R"
      },
      "source": [
        "### Test Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XNrVdp0dWUO"
      },
      "source": [
        "# Set the temperature (usually it is in (0, 1])\r\n",
        "temperature = 0.85\r\n",
        "\r\n",
        "# Set the starting text\r\n",
        "next_char = tf.constant(['[Intro]'])\r\n",
        "\r\n",
        "# Set how many characters to generate\r\n",
        "length = 1000\r\n",
        "\r\n",
        "# Create the model to test\r\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=temperature)\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "states = None\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "# Generate lyrics\r\n",
        "for n in range(length):\r\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
        "    result.append(next_char)\r\n",
        "\r\n",
        "result = tf.strings.join(result)\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "# Print generated lyrics\r\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*72)\r\n",
        "print(f\"\\nRun time: {end - start}\")\r\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-IisGv2eJhH"
      },
      "source": [
        "## Finetuning\r\n",
        "To finetune training on a smaller dataset, we can train on the original model archeticture we used or we freeze some layers. As an example, there is Model Four with its two LSTM layers frozen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np1_UNzKqx-G",
        "outputId": "5f0d2609-6f36-4592-dabd-79c0b4d2c466"
      },
      "source": [
        "# Freeze the LSTM Layers\r\n",
        "class ModelFour_Freeze(keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\r\n",
        "        super().__init__(self)\r\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "        self.lstm_one = tf.keras.layers.LSTM(rnn_units,\r\n",
        "                                    return_sequences=True, \r\n",
        "                                    return_state=True,\r\n",
        "                                    trainable=False # freeze layer\r\n",
        "        )\r\n",
        "        self.lstm_two = tf.keras.layers.LSTM(rnn_units,\r\n",
        "                                    return_sequences=True, \r\n",
        "                                    return_state=True,\r\n",
        "                                    trainable=False # freeze layer\r\n",
        "        )\r\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\r\n",
        "        x = inputs\r\n",
        "        x = self.embedding(x, training=training)\r\n",
        "        if states is None:\r\n",
        "            states = [None, None]\r\n",
        "        x, h, c = self.lstm_one(x, initial_state=states[0], training=training)\r\n",
        "        states[0] = [h,c]\r\n",
        "        x, h, c = self.lstm_two(x, initial_state=states[1], training=training)\r\n",
        "        states[1] = [h,c]\r\n",
        "        x = self.dense(x, training=training)\r\n",
        "\r\n",
        "        if return_state:\r\n",
        "            return x, states\r\n",
        "        else: \r\n",
        "            return x\r\n",
        "\r\n",
        "model = ModelFour_Freeze(\r\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\r\n",
        "    embedding_dim=EMBEDDING_DIM,\r\n",
        "    rnn_units=RNN_UNITS)\r\n",
        "\r\n",
        "for input_batch, target_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 300, 86) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8964KkNMeV_T"
      },
      "source": [
        "### Get new text to finetune against"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-58MoTh9dKMY"
      },
      "source": [
        "path_to_file = \"/content/ShawnMendes.txt\"\r\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n",
        "\r\n",
        "chars = tf.strings.unicode_split(text, input_encoding='UTF-8')\r\n",
        "ids = ids_from_chars(chars)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4cj3MlJeZ_-"
      },
      "source": [
        "### Create new dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7L788AReg_K"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(ids)\r\n",
        "\r\n",
        "sequences = ids_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)\r\n",
        "dataset = (\r\n",
        "    dataset\r\n",
        "    .shuffle(BUFFER_SIZE)\r\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFxK_4umhIGJ"
      },
      "source": [
        "### Load previous weights (if necessary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXrk9oymhMI-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e81cb9b-8150-468f-cb14-c0c6357ecdb8"
      },
      "source": [
        "# # LOAD PREVIOUS WEIGHTS IF YOU HAVE THEM\r\n",
        "checkpoint_dir = './'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_30\") # only if you want to load specific weights\r\n",
        "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) \r\n",
        "model.load_weights(checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fceff8e3048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnAljRh_euuu"
      },
      "source": [
        "### Train on smaller dataset\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbRZOt8vevBD"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "model.compile(optimizer='adam', loss=loss)\r\n",
        "\r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix, \r\n",
        "    save_weights_only=True, \r\n",
        "    save_freq=5 * (len(sequences) // BATCH_SIZE),\r\n",
        "    verbose=1\r\n",
        ") # save_freq for how many checkpoints (every 5 epochs)\r\n",
        "\r\n",
        "history_finetune = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])\r\n",
        "plt.plot(history.history['loss'] + finetune.history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2INfQCcgRjT"
      },
      "source": [
        "### Test again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igoyX89MgTbp"
      },
      "source": [
        "# Set the temperature (usually it is in (0, 1])\r\n",
        "temperature = 0.85\r\n",
        "\r\n",
        "# Set the starting text\r\n",
        "next_char = tf.constant(['[Intro]'])\r\n",
        "\r\n",
        "# Set how many characters to generate\r\n",
        "length = 1000\r\n",
        "\r\n",
        "# Create the model to test\r\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=temperature)\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "states = None\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "# Generate lyrics\r\n",
        "for n in range(length):\r\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
        "    result.append(next_char)\r\n",
        "\r\n",
        "result = tf.strings.join(result)\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "# Print generated lyrics\r\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*72)\r\n",
        "print(f\"\\nRun time: {end - start}\")\r\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkxZgvJRBR-o"
      },
      "source": [
        "## Saving weights\r\n",
        "We will save every 5th file for weights, along with the checkpoint\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRE0lS9UBjVR",
        "outputId": "8dadef84-cd58-44a6-ef46-ef63f4f6593a"
      },
      "source": [
        "!zip model.zip training_checkpoints/ckpt_*0.* training_checkpoints/ckpt_*5.* training_checkpoints/checkpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: training_checkpoints/ckpt_20.data-00000-of-00001 (deflated 7%)\n",
            "  adding: training_checkpoints/ckpt_20.index (deflated 66%)\n",
            "  adding: training_checkpoints/ckpt_30.data-00000-of-00001 (deflated 7%)\n",
            "  adding: training_checkpoints/ckpt_30.index (deflated 66%)\n",
            "  adding: training_checkpoints/ckpt_25.data-00000-of-00001 (deflated 7%)\n",
            "  adding: training_checkpoints/ckpt_25.index (deflated 66%)\n",
            "  adding: training_checkpoints/checkpoint (deflated 38%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}